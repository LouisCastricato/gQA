{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gQA",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+HoDx2rXO+EZ/BrzsJo5w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LouisCastricato/gQA/blob/master/gQA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89icpDWFjATM",
        "colab_type": "text"
      },
      "source": [
        "#Boring setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OBPceSjPKr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mport os\n",
        "\n",
        "os.environ['mdl'] = \"gqa_m8_200\"\n",
        "!pip install git+https://github.com/ildoonet/pytorch-gradual-warmup-lr.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKlrXEPYRxfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /home/\n",
        "!git clone https://github.com/LouisCastricato/gQA.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIudEmVKiwCK",
        "colab_type": "text"
      },
      "source": [
        "#Download the dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHmxxiau8_IF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /home/\n",
        "import boto3\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "\n",
        "s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "s3.download_file('gqa-dataset', 'dataset.tar.gz', 'dataset.tar.gz')\n",
        "\n",
        "!tar zxf dataset.tar.gz -C '/home/'\n",
        "%mv .vector_cache/ gQA/\n",
        "%mv hotpotqa_data/ gQA/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-HpGz1Eiz4q",
        "colab_type": "text"
      },
      "source": [
        "#Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8s6_Chae172",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /home/gQA\n",
        "%mkdir models\n",
        "%cd models/\n",
        "\n",
        "%mkdir $mdl\n",
        "!touch $mdl\"/\"$mdl\".model\"\n",
        "!touch $mdl\"/\"$mdl\"_checkpoint.model\"\n",
        "!touch $mdl\"/\"$mdl\"_best.model\"\n",
        "!touch $mdl\"/\"$mdl\"_loss_graph.csv\"\n",
        "\n",
        "%cd ..\n",
        "!python3 train_gqa.py --train_qa --task hotpotqa --data_path hotpotqa_data/ --file_limit 90400 \\\n",
        "--save_path \"models/\"$mdl\"/\"$mdl --span \\\n",
        "--max_sentence_len 25 --split --model 'lstm-ggcn-lstm' \\\n",
        "--postgcn_attn --global_self_attn --save "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsv7sOH_i2u7",
        "colab_type": "text"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHefjtHmi5ow",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python3 train_gqa.py --train_qa --task hotpotqa --data_path hotpotqa_data/ --file_limit 90400 \\\n",
        "--save_path \"models/\"$mdl\"/\"$mdl\"_best\" --span \\\n",
        "--max_sentence_len 25 --split --model 'lstm-ggcn-lstm' \\\n",
        "--postgcn_attn --global_self_attn --load --test"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}